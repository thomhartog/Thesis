{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pandas.core.common import flatten\n",
    "import numpy as np\n",
    "import more_itertools as mit\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDSF:\n",
    "    \n",
    "    def __init__(self, file, sheet1, sheet2): \n",
    "        # load file\n",
    "        df = pd.read_excel(io=file, sheet_name=sheet1)\n",
    "        # rename colums\n",
    "        df.columns = ['0', 'Knowledge Area Groups', 'KAG code', 'KA code', 'Knowledge Areas', 'KU code', 'Suggested Knowledge Units', '7', 'Mapping to CCS2012']\n",
    "        # drop NaN rows\n",
    "        df['0'] = df.isnull().all(axis=1).cumsum().dropna()\n",
    "        \n",
    "        # split dataframe on NaNs\n",
    "        self.d = d = {n: df.iloc[rows][1:] for n, rows in df.groupby('0').groups.items() if (len(df.iloc[rows]) > 1)}\n",
    "        \n",
    "        # load KAG's, KA's and KU's for the file\n",
    "        self.kags = set([list(d[key]['Knowledge Area Groups'])[0] for key in self.d])\n",
    "        self.kas = [list(self.d[key]['Knowledge Areas'])[0] for key in self.d]\n",
    "        self.kus = [x for x in list(flatten([list(self.d[key]['Suggested Knowledge Units']) for key in self.d])) if x == x]\n",
    "        \n",
    "        # load codes\n",
    "        self.kagcodes = [list(df['KAG code'])[0].split('/')[0] for df in self.d.values()]\n",
    "        self.kacodes = [list(df['KA code'])[0] for df in self.d.values()]\n",
    "        self.kucodes = [x for x in list(flatten([list(self.d[key]['KU code']) for key in self.d])) if x == x]\n",
    "        \n",
    "        # load competences\n",
    "        df_comp = pd.read_excel(io=file, sheet_name=sheet2)\n",
    "        df_comp.columns = df_comp.iloc[0]\n",
    "        \n",
    "        # split dataframe on knowledge area group\n",
    "        idx_comp_kags = [list(df_comp.columns).index(item) for item in list(df_comp.columns) if '(' in item]\n",
    "        splitted_df = []\n",
    "        for idx, i in enumerate(idx_comp_kags):\n",
    "            if idx != len(idx_comp_kags) - 1:\n",
    "                l = list(range(i, idx_comp_kags[idx+1]))\n",
    "            else:\n",
    "                l = list(range(i, len(df_comp.columns)))\n",
    "            splitted_df.append(df_comp.iloc[:,l].reset_index(drop=True))\n",
    "        \n",
    "        # split resulting dataframe on knowledge area\n",
    "        self.splitted_df = []\n",
    "        for df in splitted_df:\n",
    "            # find the idx with nans\n",
    "            boolnanlist = df.iloc[:,[1,2]].isnull()\n",
    "            allnanlist = list(np.where(boolnanlist.all(axis=1))[0])\n",
    "\n",
    "            # with multiple consecutive nans, only keep the highest idx\n",
    "            nanlist = [max(list(map(itemgetter(1), g))) for k, g in groupby(enumerate(allnanlist), lambda ix: ix[0]-ix[1])]\n",
    "            if not df.iloc[-1].isnull().values.all():\n",
    "                nanlist = nanlist + [len(df)]\n",
    "\n",
    "            # split the dataframe on knowledge area\n",
    "            splitted = [df.iloc[nanlist[n]:nanlist[n+1]].dropna(how='all').reset_index(drop=True) for n in range(len(nanlist)-1)]\n",
    "            self.splitted_df.append(splitted)\n",
    "        \n",
    "        # make dicts with ka and ku as key and competence as value\n",
    "        self.ka_dict = defaultdict(list)\n",
    "        self.ku_dict = defaultdict(list)\n",
    "        for group in self.splitted_df:\n",
    "            for area in group:\n",
    "                for index, row in area.iloc[1:].iterrows():\n",
    "                    competence = row.iloc[0]\n",
    "                    ka = row.iloc[1]\n",
    "                    ku = row.iloc[2]\n",
    "                    ku2 = None\n",
    "                    try:\n",
    "                        ku2 = row.iloc[3]\n",
    "                    except:\n",
    "                        pass\n",
    "                    if pd.notna(ka):\n",
    "                        ka = ka.split(',')\n",
    "                        for k in ka:\n",
    "                            self.ka_dict[k.strip()].append(competence)\n",
    "                    if pd.notna(ku):\n",
    "                        ku = ku.split(',')\n",
    "                        for k in ku:\n",
    "                            self.ku_dict[k.strip()].append(competence)\n",
    "                    if pd.notna(ku2):\n",
    "                        ku2 = ku2.split(',')\n",
    "                        for k in ku2:\n",
    "                            self.ku_dict[k.strip()].append(competence)\n",
    "                            \n",
    "    def switch(self, input_):\n",
    "        \"\"\" switch between kagcode and kacode \"\"\"\n",
    "        if 'k' in input_.lower():\n",
    "            idx = self.kacodes.index(input_)\n",
    "            return self.kagcodes[idx]\n",
    "        else:\n",
    "            idx = self.kagcodes.index(input_)\n",
    "            return self.kacodes[idx]\n",
    "        \n",
    "    def get_ka_from_kag(self, kag):\n",
    "        kaset = set()\n",
    "        for key in self.d:\n",
    "            if list(self.d[key]['Knowledge Area Groups'])[0] == kag:\n",
    "                kaset.add(list(self.d[key]['Knowledge Area'])[0])\n",
    "        return kaset\n",
    "        \n",
    "    def get_ku_from_kag(self, kag):\n",
    "        kulist = []\n",
    "        for key in self.d:\n",
    "            if list(self.d[key]['Knowledge Area Groups'])[0] == kag:\n",
    "                for item in list(self.d[key]['Suggested Knowledge Units']):\n",
    "                    if item == item:\n",
    "                        kulist.append(item)\n",
    "        return kulist\n",
    "       \n",
    "    def get_kag_from_ka(self, ka):\n",
    "        kagset = set()\n",
    "        for key in self.d:\n",
    "            if list(self.d[key]['Knowledge Areas'])[0] == ka:\n",
    "                kagset.add(list(self.d[key]['Knowledge Area Groups'])[0])\n",
    "        return kagset\n",
    "            \n",
    "    def get_ku_from_ka(self, ka):\n",
    "        kulist = []\n",
    "        for key in self.d:\n",
    "            if list(self.d[key]['Knowledge Areas'])[0] == ka:\n",
    "                for item in list(self.d[key]['Suggested Knowledge Units']):\n",
    "                    if item == item:\n",
    "                        kulist.append(item)\n",
    "        return kulist\n",
    "        \n",
    "    def get_kag_from_ku(self, ku):\n",
    "        kagset = set()\n",
    "        for key in self.d:\n",
    "            if ku in list(self.d[key]['Suggested Knowledge Units']):\n",
    "                kagset.add(list(self.d[key]['Knowledge Area Groups'])[0])\n",
    "        return kagset\n",
    "    \n",
    "    def get_ka_from_ku(self, ku):\n",
    "        kaset = set()\n",
    "        for key in self.d:\n",
    "            if ku in list(self.d[key]['Suggested Knowledge Units']):\n",
    "                kaset.add(list(self.d[key]['Knowledge Areas'])[0])\n",
    "        return kaset\n",
    "    \n",
    "    def get_ku_from_kacode(self, code_ka):\n",
    "        kulist = []\n",
    "        for key in self.d:\n",
    "            if list(self.d[key]['KA code'])[0].split('/')[0] == code_ka:\n",
    "                for item in list(self.d[key]['Suggested Knowledge Units']):\n",
    "                    if item == item:\n",
    "                        kulist.append(item)\n",
    "        return kulist\n",
    "    \n",
    "    def get_ku_from_kucode(self, code_ku):\n",
    "        if code_ku[2] != '0':\n",
    "            code_ku = f'{code_ku[:2]}0{code_ku[2:]}'\n",
    "        idx = self.kusc.index(code_ku)\n",
    "        return self.kus[idx]\n",
    "    \n",
    "    def get_kacode_from_ka(self, ka):\n",
    "        for key in self.d:\n",
    "            if list(self.d[key]['Knowledge Areas'])[0] == ka:\n",
    "                return list(self.d[key]['KA code'])[0]\n",
    "    \n",
    "    def get_kucode_from_ku(self, ku):\n",
    "        codeset = set()\n",
    "        for key in self.d:\n",
    "            if ku in list(self.d[key]['Suggested Knowledge Units']):\n",
    "                idx = list(self.d[key]['Suggested Knowledge Units']).index(ku)\n",
    "                codeset.add(list(self.d[key]['KU code'])[idx])\n",
    "        return codeset\n",
    "\n",
    "    def get_comp_from_ka(self, ka):\n",
    "        code = self.get_code_from_ka(ka)\n",
    "        for key in self.ka_dict:\n",
    "            if key == code:\n",
    "                return self.ka_dict[key]\n",
    "        return None\n",
    "            \n",
    "    def get_comp_from_ku(self, ku):\n",
    "        code = self.get_code_from_ku(ku)\n",
    "        for key in list(self.ku_dict):\n",
    "            mod_key = key.replace('0', '', 1)\n",
    "            for cod in code:\n",
    "                if mod_key == cod:\n",
    "                    return self.ku_dict[key]\n",
    "        return None\n",
    "    \n",
    "    def get_comp_from_code(self, code):\n",
    "        if 'a' in code.lower():\n",
    "            return self.ka_dict[code]\n",
    "        elif 'u' in code.lower():\n",
    "            if code[2] != '0':\n",
    "                code = f'{code[:2]}0{code[2:]}'\n",
    "                return self.ku_dict[code]\n",
    "            else:\n",
    "                return self.ku_dict[code]\n",
    "        else:\n",
    "            print('please input valid id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = EDSF(file='working_dsci-dsp-competences-to-bok-v03.xlsx', \n",
    "                      sheet1='DS-BoK - release 3 (reference)',\n",
    "                      sheet2='CF-DS Data Sci Maping to BoK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj.ku_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_competence_with_id(obj):\n",
    "    all_c = []\n",
    "    for group in obj.splitted_df:\n",
    "        for area in group:\n",
    "            for index, row in area.iloc[1:].iterrows():\n",
    "                idx = area.iloc[0][0].split('–')[0].split('-')[0].strip()\n",
    "                iden = 'C_' + idx + f'.{index-1}'\n",
    "                competence = row.iloc[0]\n",
    "                all_c.append([iden, competence])\n",
    "    return all_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_competence_with_id2(obj):\n",
    "    all_c = []\n",
    "    for group in obj.splitted_df:\n",
    "        for area in group:\n",
    "            value = list(area.iloc[:,0])\n",
    "            iden = value[0].split('–')[0].split('-')[0].strip()\n",
    "            competence = ' '.join(value[1:])\n",
    "            all_c.append([iden, competence])\n",
    "    return all_c\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competences = all_competence_with_id2(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DSDA',\n",
       "  'Use appropriate data analytics and statistical techniques on available data to discover new relations and deliver insights into research problem or organizational processes  and support decision-making. '],\n",
       " ['DSDA01',\n",
       "  'Effectively use variety of data analytics techniques, such as Machine Learning (including supervised, unsupervised, semi-supervised learning), Data Mining, Prescriptive and Predictive Analytics, for complex data analysis through the whole data lifecycle'],\n",
       " ['DSDA02',\n",
       "  'Apply designated quantitative techniques, including statistics, time series analysis, optimization, and simulation to deploy appropriate models for analysis and prediction'],\n",
       " ['DSDA03',\n",
       "  'Identify, extract, and pull together available and pertinent heterogeneous data, including  modern data sources such as social media data, open data, governmental data, verify data quality'],\n",
       " ['DSDA04',\n",
       "  'Understand and use different performance and accuracy metrics for model validation in analytics projects, hypothesis testing, and information retrieval'],\n",
       " ['DSDA05',\n",
       "  'Develop required data analytics for organizational tasks,  integrate data analytics and processing applications into organization workflow and business processes to enable agile decision making'],\n",
       " ['DSDA06',\n",
       "  'Visualise results of data analysis, design dashboard and use storytelling methods'],\n",
       " ['DSDM',\n",
       "  'Develop and implement data management strategy for data collection, storage, preservation, and availability for further processing, ensure compliance with FAIR data principles.'],\n",
       " ['DSDM01',\n",
       "  'Develop and implement data management and governance strategy, in particular, in the form of Data Governance Policy and Data Management Plan (DMP) Ensure compliance with standards and best practices in Data Governance and Data Management'],\n",
       " ['DSDM02',\n",
       "  'Develop and implement relevant data models, define metadata using common standards and practices for different data sources in a variety of scientific and industry domains. Ensure metadata compliance with FAIR requirements  Be familiar with the metadata management tools'],\n",
       " ['DSDM03',\n",
       "  'Integrate heterogeneous data from multiple sources and provide them for further analysis and use  Perform data preparation and cleaning  Match/transfer data models of individual datasets'],\n",
       " ['DSDM04',\n",
       "  'Maintain historical information on data handling, including reference to published data and corresponding data sources   Publish data, metadata and related metrics  Perform and maintain data archiving Develop necessary archiving policy, comply with Open Science and Open Access policies if applicable Perform data provenance and ensure continuity through the whole data lifecycle, ensure data provenance '],\n",
       " ['DSDM05',\n",
       "  'Develop policy and metrics for data quality management (e.g. Altmetrix), maintain data quality and compliance to standards, perform data curation. Interact/Collaborate with data providers and data owners to ensure data quality'],\n",
       " ['DSDM06',\n",
       "  'Develop and manage/supervise policies on data protection, privacy, IPR and ethical issues in data management, address legal issues if necessary. Ensure GDPR compliance in data management and access. Develop data access policies and coordinate their implementation and monitoring, including security breaches handling.'],\n",
       " ['DSDM07*',\n",
       "  'Manage Data Management/Data Stewards team, coordinate related activity between organisational departments, external stakeholder to fulfill Data Governance policy requirements, provide advice and training to staff. Define domain/organisation specific data management requirements, communicate to all departments and supervise/coordinate their implementation. Coordinate/supervise data acquisition.'],\n",
       " ['DSDM08*',\n",
       "  'Develop organisational policy and coordinate activities for sustainable implementation of the FAIR data principles and Open Science, define corresponding requirements to data infrastructure and tools, ensure organisational awareness.'],\n",
       " ['DSDM09*',\n",
       "  'Specify requirements to and supervise the organisational infrastructure for data management and (and archiving), maintain the park for data management tools, provide support to staff (researchers or business developers), coordinate solving problems.'],\n",
       " ['DSENG',\n",
       "  'Use engineering principles and modern computer technologies to research, design, implement new data analytics applications; develop experiments, processes, instruments, systems, infrastructures to support data handling during the whole data lifecycle. '],\n",
       " ['DSENG01',\n",
       "  'Use engineering principles (general and software) to research, design, develop and implement new instruments and applications for data collection, storage, analysis and visualisation'],\n",
       " ['DSENG02',\n",
       "  'Develop and apply computational and data driven solutions to domain related problems using wide range of data analytics platforms, with a special focus on Big Data technologies for large datasets and cloud based data analytics platforms'],\n",
       " ['DSENG03',\n",
       "  'Develop and prototype specialised data analysis applications, tools and supporting infrastructures for data driven scientific, business or organisational workflow; use distributed, parallel, batch and streaming processing platforms, including online and cloud based solutions for on-demand provisioned and scalable services Develop new tools and applications, ensure support of the data FAIRness requirements by existing and new tools and applications.'],\n",
       " ['DSENG04',\n",
       "  'Develop, deploy and operate data infrastructure, including data storage and processing facilities, using different distributed and cloud based platforms. Implement requirements for data storage facilities to comply with the data management policies and FAIR data principles in particular.'],\n",
       " ['DSENG05',\n",
       "  'Consistently apply data security mechanisms and controls at each stage of the data processing, including data anonymisation, privacy and IPR protection, ensure standards and corresponding data protection regulation compliance, in particular GDPR. Define and implement (coordinate) data access policies for different stakeholders and organisational roles'],\n",
       " ['DSENG06',\n",
       "  'Design, build, operate relational and non-relational databases (SQL and NoSQL), integrate them with the modern Data Warehouse solutions, ensure effective ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform), OLTP, OLAP processes for large datasets Define, implement and maintain data model, reference data, master data definitions, implement consistent metadata'],\n",
       " ['DSRMP',\n",
       "  'Create new understandings and capabilities by using the scientific method (hypothesis, test/artefact, evaluation) or similar engineering methods to discover new approaches to create new knowledge and achieve research or organisational goals. Use collected scientific facts and collected data to advance your research. '],\n",
       " ['DSRMP01',\n",
       "  'Create new understandings, discover new relations by using the research methods (including hypothesis, artefact/experiment, evaluation) or similar engineering research and development methods'],\n",
       " ['DSRMP02',\n",
       "  'Direct systematic study toward the understanding of the observable facts, and discovers new approaches to achieve research or organisational goals'],\n",
       " ['DSRMP03',\n",
       "  'Analyse domain related research process model, identify and analyse available data to identify research questions and/or organisational objectives and formulate sound hypothesis  Link domain related concepts and models to general/abstract Data Science concepts and models'],\n",
       " ['DSRMP04',\n",
       "  'Undertake creative work, making systematic use of investigation or experimentation, to discover or revise knowledge of reality, and use this knowledge to devise new applications (data driven), contribute to the development of organizational or project objectives'],\n",
       " ['DSRMP05',\n",
       "  'Design experiments which include data collection (passive and active) for hypothesis testing and problem solving Work with Data Science, Data Stewardship and data infrastructure teams to develop project/research goals.'],\n",
       " ['DSRMP06',\n",
       "  'Develop and guide data driven projects, including project planning, experiment design, data collection and handling'],\n",
       " ['DSDK',\n",
       "  'Use domain knowledge (scientific or business) to develop relevant data analytics applications; adopt general Data Science methods to domain specific data types and presentations, data and process models, organisational roles and relations'],\n",
       " ['DSBA01',\n",
       "  'Analyse information needs, assess existing data and suggest/identify new data required for specific business context to achieve organizational goal, including using social network and open data sources Data management and Quality Assurance of organisational data assets'],\n",
       " ['DSBA02',\n",
       "  'Operationalise fuzzy concepts to enable key performance indicators measurement to validate the business analysis, identify and assess potential challenges Specify requirements/develop data models for organisational data'],\n",
       " ['DSBA03',\n",
       "  'Deliver business focused analysis using appropriate BA/BI methods and tools, identify business impact from trends; make business case as a result of organisational data analysis and identified trends Ensure data availability and quality for BA/BI needs'],\n",
       " ['DSBA04',\n",
       "  'Analyse opportunity and suggest the use of historical data available at organisation for organizational processes optimization Coordinate implementation of FAIR data principles for collected data, ensure proper lineage and provenance of collected data'],\n",
       " ['DSBA05',\n",
       "  'Analyse customer relations data to optimise/improve interaction with the specific user groups or in the specific business sectors'],\n",
       " ['DSBA06',\n",
       "  'Analyse multiple data sources for marketing purposes; identify effective marketing actions'],\n",
       " ['DSBA07',\n",
       "  'Coordinate intra organisational activities related to data analytics, data management and data provenance/lineage along all data flow stages, ensure data FAIRness']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_competences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_units(obj):\n",
    "    kalist = []\n",
    "    for code in obj.kacodes:\n",
    "        kus = obj.get_ku_from_kacode(code)\n",
    "        flat = kus[0]\n",
    "        for item in kus[1:]:\n",
    "            item = item.replace('and'\n",
    "                                , '')\n",
    "            flat += (' ' + item)\n",
    "        kalist.append([code, flat])\n",
    "    return kalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['KA01.01',\n",
       "  'Probability & Statistics Statistical paradigms (regression, time series, dimensionality, clusters) Probabilistic representations (causal networks, Bayesian analysis, Markov nets) Frequentist  Bayesian statistics Probabilistic reasoning Exploratory  confirmatory data analysis Quantittaive analytics Qualitative Analytics Data preparation  preprocessing Performance analysis Markov models, markov networks Operations research Information theory Discrete Mathematics  Graph Theory Mathematical analysis Mathematical software  tools'],\n",
       " ['KA01.02',\n",
       "  'Machine Learning theory and algorithms Supervised Machine Learning Unsupervised Machine Learning Reinforced learning Classification methods Design  Analysis of Algorithms Game Theory & Mechanism design Artificial Intelligence Statistical paradigms (regression, time series, dimensionality, clusters) Probabilistic representations (causal networks, Bayesian analysis, Markov nets) Frequentist  Bayesian statistics Probabilistic reasoning Performance analysis'],\n",
       " ['KA01.03',\n",
       "  'Data mining and knowledge discovery Knowledge Representation  Reasoning CRISP-DM  data mining stages Anomaly Detection Time series analysis Feature selection, Apriori algorithm Graph data analytics Performance analysis Machine Learning theory  algorithms Supervised Machine Learning Unsupervised Machine Learning Reinforced learning Classification methods'],\n",
       " ['KA01.04 ',\n",
       "  'Text analytics including statistical, linguistic, and structural techniques to analyse structured and unstructured data Data mining  text analytics Natural Language Processing Predictive Models for Text Retrieval  Clustering of Documents Information Extraction Sentiments analysis'],\n",
       " ['KA01.05',\n",
       "  'Predictive modeling and analytics Inferential  predictive statistics Machine Learning for predictive analytics  Prescriptive Analytics Regression  Multi Analysis Generalised linear models  Time series analysis  forecasting Deploying  refining predictive models'],\n",
       " ['KA01.06',\n",
       "  'Modelling and simulation theory and techniques (general and domain oriented) Operations research  optimisation Large scale modelling  simulation systems Network oprtimisation Risk simulation  queueing '],\n",
       " ['KA02.01',\n",
       "  'Computer systems organisation for Big Data applications, CAP, BASE and ACID theorems Parallel  Distributed Computer Architecture High Performance  Cloud Computing Clouds  scalable computing Cloud based Big Data platforms  services Big Data (large scale) storage  filesystems (HDFS, Ceph, etc) NoSQL databases Computer networks for high-performance computing  Big Data infrastructure Computer networks: architectures  protocols Big Data Infrastructure management  operation'],\n",
       " ['KA02.02',\n",
       "  'Big Data Infrastructure: services and components, including data storage infrastructure Big Data analytics platforms  tools (including Hadoop, Spark,  cloud based Big Data services) Large scale cloud based storage  data management  Cloud based applications  services operation  management Big Data  cloud based systems design  development Data processing models (batch, steaming, parallel) Enterprise information systems Data security  protection'],\n",
       " ['KA02.03',\n",
       "  'Cloud Computing  architecture and services Cloud Computing Engineering (infrastructure  services design, management  operation) Cloud based applications  services operation  management'],\n",
       " ['KA02.04',\n",
       "  'Infrastructure, applications and data security Data encryption  key management, bockchain based technologies  Access Control  Identiy Management Security services management, including compliance  certification Data anonymisation Data privacy'],\n",
       " ['KA02.05',\n",
       "  'Big Data systems organisation and design Big Data algorithms for large scale data processing Big Data Analytics Big Data analytics platforms  tools (including Hadoop, Spark,  cloud based Big Data services) Big Data algorithms for data ingest, pre-processing,  visualisation Big Data systems for application domains Big Data software (systems) architectures Requirements engineering  software systems development Large  ultra-large scale software systems organisation DevOps  cloud enabled applications development Big Data Infrastructure management  operation'],\n",
       " ['KA02.06',\n",
       "  'Data analytics, data handling software requirements and design Applications engineering management Software engineering models  methods Software quality assurance Programming languages for Big Data analytics: R, python, Pig, Hive, others Models  languages for complex interlinked  data presentation  visualisation Agile development methods, platforms  tools DevOps  continuous deployment  improvement paradigm'],\n",
       " ['KA02.07',\n",
       "  'Decision Analysis and Decision Support Systems Predictive analytics  predictive forecasting Data Analysis  statistics Data warehousing  Data Mining Data Mining Multimedia information systems Enterprise information systems Collaborative  social computing systems  tools'],\n",
       " ['KA03.01',\n",
       "  'Data type, data type registries, data formats, PID  Metadata, metadata formats Data Lifecycle Management Data infrastructure  Data Factories Research data infrastructure, Open Science, Open Data, Open Access, ORCID Compliance  certification in Data infrastructure (data archives) Ethical principle  data privacy FAIR (Findable, Accessible, Interoperable, ) principles in Data Management'],\n",
       " ['KA03.02',\n",
       "  'Data architectures (OLAP, OLTP, ETL) Data Modelling, Databases  Database Management Systems Data structures Data Models  Query Languages Database design  models Database administration Data warehouses Middleware for databases'],\n",
       " ['KA03.03',\n",
       "  'Data management, including Reference and Master Data Data Warehousing  Business Intelligence Data storage  operations Data archives/storage compliance  certification Metadata, linked data, provenance Data infrastructure, data registries  data factories Data security  protection Data backup Data anonymisation Data Privacy'],\n",
       " ['KA03.04',\n",
       "  'Data governance, data quality, data Integration and Interoperability Data Management Planning Data Management Policy Data interoperability Data curation Data provenance Responsible data use, data privacy, ethical principles, IPR, legal issues'],\n",
       " ['KA03.05',\n",
       "  'Big Data storage infrastructure and operations Storage architectures, distributed files systems (HDFS, Ceph, Lustre, Gluster, etc) Data storage redundancy  backup Data factories, data pipelines Cloud based storage, Data Lakes'],\n",
       " ['KA03.06',\n",
       "  'Digital libraries and archives organisation Information Retrieval Data curation  provenance Search Engines technologies'],\n",
       " ['KA04.01',\n",
       "  'Research methodology, paradigms and research cycle Modelling  experiment planning Data selection  quality evaluation Data lifecycle Use cases analysis: research infrastructures  projects Research data management plan  ethical issues'],\n",
       " ['KA04.02',\n",
       "  'Project Integration Management Project Scope Management Project Quality Project Risk Management'],\n",
       " ['KA05.01',\n",
       "  'Business Analytics and Business Intelligence: Data, Models (statistical) and Decisions Data driven Customer Relations Management (CRP), User Experience (UX) requirements  design Operations Analytics Business Process Optimization  Data  Warehouses technologies, data integration  analytics Data driven marketing technologies Business Analytics Capstone Econometrics methods  application for Business Analytics Cognitive technologies for Businsess Analytics'],\n",
       " ['KA05.02',\n",
       "  'Business processes and operations Project scope  risk management Business Analysis Planning  Monitoring Requirements Analysis  Design Definition Requirements Life Cycle Management (from inception to retirement) Solution Evaluation  improvements recommendation Agile Data Driven methodologies, processes  enterprises Use cases analysis: business  industry']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kagcode_with_ka = knowledge_units(obj)\n",
    "kagcode_with_ka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
